# The directory containing the pdfs to convert
pdf_dir: /lus/flare/projects/FoundEpidem/siebenschuh/adaparse_data/input/small-pdf-dataset

# The directory to place the converted pdfs in
out_dir: /lus/flare/projects/FoundEpidem/siebenschuh/adaparse_data/output/adaparse_small_test_by_doc_with_fillin

# AdaParse *requires* PDFs as zipped input
iszip: true

# The settings for the pdf parser
parser_settings:
  # The name of the parser to use
  name: adaparse
  # Max. proportion of high-quality parser (Nougat): throughput-sensitive
  alpha: 0.5
  # Prediction/delegation-granularity (either document-wise or page-wise)
  prediction_mode: by_doc

  # Recommended batch size of pages (not pdfs) is 10, maximum that fits into A100 40GB.
  batchsize: 5
  # Path to download the checkpoint to. if already exists, will use existing.
  checkpoint: "/lus/flare/projects/FoundEpidem/siebenschuh/adaparse_data/meta/nougat/checkpoint"
  # Set mmd_out to null if you don't want to write mmd files as a byproduct.
  mmd_out: "/lus/flare/projects/FoundEpidem/siebenschuh/adaparse_data/meta/nougat/mmd2"
  # If set to false, a will skip the pdfs that already have a parsed mmd file in mmd_out
  recompute: false
  # If set to true, text of pages with "[MISSING...]" will be filled by PyMuPDF
  fill_missing_pages: true
  # Set to true if you want to use fp32 instead of bfloat16 (false is recommended)
  full_precision: false
  # If set to true, output text will be formatted as a markdown file.
  markdown: true
  # Preempt processing a paper if mode collapse causes repetitions (true is recommended)
  skipping: true
  # Path for the nougat-specific logs for the run.
  nougat_logs_path: "/lus/flare/projects/FoundEpidem/siebenschuh/adaparse_data/meta/nougat/logs"

  # AdaParse Regression
  prediction_model_dir: "/lus/flare/projects/FoundEpidem/siebenschuh/adaparse_data/prediction"
  # The HF identifier (if None, inferred by `prediction_mode`)
  prediction_model_name: None
  # The batch size for the classifier.
  batch_size: 256
  # The maximum length of the input text (in characters).
  max_character_length: 3200
  # The number of data workers for the classifier.
  num_data_workers: 1
  # Whether to pin memory for the classifier.
  pin_memory: true

# The compute settings for the workflow
compute_settings:
  # The name of the compute platform to use
  name: aurora
  # The number of compute nodes to use
  num_nodes: 1
  # Make sure to update the path to your conda environment and HF cache
  worker_init: "source /home/siebenschuh/AdaParse/scripts/standup_aurora.sh"
  # The scheduler options to use when submitting jobs
  scheduler_options: "#PBS -l filesystems=home:flare"
  # Make sure to change the account to the account you want to charge
  account: FoundEpidem
  # The HPC queue to submit to
  queue: debug
  # The amount of time to request for your job
  walltime: 00:12:00
