{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "50b6f29a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "import json, gzip\n",
    "from typing import List, Dict, Iterable, Union\n",
    "import pandas as pd\n",
    "from typing import List, Dict, Any"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "b94ff6b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def _iter_jsonl(path: Path) -> Iterable[Dict]:\n",
    "    \"\"\"Yield dicts from a single .jsonl or .jsonl.gz file.\"\"\"\n",
    "    opener = gzip.open if path.suffix == \".gz\" or path.name.endswith(\".jsonl.gz\") else open\n",
    "    mode = \"rt\" if opener is gzip.open else \"r\"\n",
    "    with opener(path, mode, encoding=\"utf-8\") as f:\n",
    "        for line in f:\n",
    "            line = line.strip()\n",
    "            if not line:\n",
    "                continue\n",
    "            obj = json.loads(line)\n",
    "            # If a writer accidentally dumped arrays/objects per line, normalize:\n",
    "            if isinstance(obj, dict):\n",
    "                yield obj\n",
    "            elif isinstance(obj, list):\n",
    "                for item in obj:\n",
    "                    if isinstance(item, dict):\n",
    "                        yield item\n",
    "                    else:\n",
    "                        raise ValueError(f\"Non-dict item in list in {path}: {type(item)}\")\n",
    "            else:\n",
    "                raise ValueError(f\"Non-dict JSON in {path}: {type(obj)}\")\n",
    "\n",
    "def load_jsonls(root: Union[str, Path]) -> List[Dict]:\n",
    "    \"\"\"Recursively load all *.jsonl / *.jsonl.gz under root into one list of dicts.\"\"\"\n",
    "    root = Path(root)\n",
    "    files = sorted(\n",
    "        [*root.rglob(\"*.jsonl\"), *root.rglob(\"*.jsonl.gz\")],\n",
    "        key=lambda p: (p.parent.as_posix(), p.name)\n",
    "    )\n",
    "    all_rows: List[Dict] = []\n",
    "    for fp in files:\n",
    "        all_rows.extend(_iter_jsonl(fp))\n",
    "    return all_rows\n",
    "\n",
    "\n",
    "def def_analyze_docs(\n",
    "    docs: List[Dict[str, Any]],\n",
    "    group_name: str,\n",
    "    clsfix_applied: bool = False\n",
    ") -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Build a DataFrame with:\n",
    "      - path: filename only (last part of doc['path'])\n",
    "      - full_text_len: len(doc['text']) if present, else 0\n",
    "      - page_count: len(doc['metadata']['page_char_idx']) if present, else None\n",
    "      - page_char_idx: list of character indices if present, else None\n",
    "      - group: provided group_name\n",
    "      - beforeclsfix: boolean flag (clsfix_applied)\n",
    "    \"\"\"\n",
    "    rows = []\n",
    "    for doc in docs:\n",
    "        raw_path = doc.get('path')\n",
    "        path = None\n",
    "        if isinstance(raw_path, str):\n",
    "            path = raw_path.split('/')[-1]  # keep filename only\n",
    "\n",
    "        text = doc.get('text', \"\")\n",
    "        full_text_len = len(text) if isinstance(text, str) else 0\n",
    "\n",
    "        page_char_idx = None\n",
    "        page_count = None\n",
    "        meta = doc.get('metadata')\n",
    "        if isinstance(meta, dict):\n",
    "            pci = meta.get('page_char_idx')\n",
    "            if isinstance(pci, (list, tuple)):\n",
    "                page_char_idx = list(pci)\n",
    "                page_count = len(page_char_idx)\n",
    "\n",
    "        rows.append({\n",
    "            'path': path,\n",
    "            'full_text_len': full_text_len,\n",
    "            'page_count': page_count,\n",
    "            'page_char_idx': page_char_idx,\n",
    "            'group': group_name,\n",
    "            'beforeclsfix': clsfix_applied,\n",
    "        })\n",
    "\n",
    "    return pd.DataFrame(\n",
    "        rows,\n",
    "        columns=['path', 'full_text_len', 'page_count', 'page_char_idx', 'group', 'beforeclsfix']\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "f8bb1c0e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Paths\n",
    "p_reference = Path('/lus/flare/projects/FoundEpidem/siebenschuh/adaparse_data/output/pymupdf_reference/parsed_pdfs')\n",
    "p_doc = Path('/lus/flare/projects/FoundEpidem/siebenschuh/adaparse_data/output/adaparse_small_test_by_doc')\n",
    "p_doc_fi = Path('/lus/flare/projects/FoundEpidem/siebenschuh/adaparse_data/output/adaparse_small_test_by_doc_with_fillin')\n",
    "p_page_fi = Path('/lus/flare/projects/FoundEpidem/siebenschuh/adaparse_data/output/adaparse_small_test_by_page_with_fillin')\n",
    "p_page = Path('/lus/flare/projects/FoundEpidem/siebenschuh/adaparse_data/output/adaparse_small_test_by_page')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c8ae49db",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load as List[document dict]\n",
    "docs_no_fillin = load_jsonls(p_doc)\n",
    "docs_with_fillin = load_jsonls(p_doc_fi)\n",
    "pages_with_fillin = load_jsonls(p_page_fi)\n",
    "pages_no_fillin = load_jsonls(p_page)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "ef6656e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# PyMuPDF\n",
    "df0 = def_analyze_docs(load_jsonls(p_reference), 'pymupdf')\n",
    "# AdaParse\n",
    "df1 = def_analyze_docs(load_jsonls(p_doc), 'doc_nofi')\n",
    "df2 = def_analyze_docs(load_jsonls(p_doc_fi), 'doc_fi')\n",
    "df3 = def_analyze_docs(load_jsonls(p_page_fi), 'page_fi')\n",
    "df4 = def_analyze_docs(load_jsonls(p_page), 'page_nofi')\n",
    "\n",
    "# merge into one\n",
    "out = pd.concat([df0, df1, df2, df3, df4], axis=0, ignore_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "73e635e1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>path</th>\n",
       "      <th>full_text_len</th>\n",
       "      <th>page_count</th>\n",
       "      <th>page_char_idx</th>\n",
       "      <th>group</th>\n",
       "      <th>beforeclsfix</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>34</th>\n",
       "      <td>13_of_20.pdf</td>\n",
       "      <td>66757</td>\n",
       "      <td>15</td>\n",
       "      <td>[0, 4490, 9751, 14423, 19943, 24580, 29637, 33...</td>\n",
       "      <td>doc_nofi</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>58</th>\n",
       "      <td>13_of_20.pdf</td>\n",
       "      <td>64526</td>\n",
       "      <td>15</td>\n",
       "      <td>[0, 4489, 9749, 14420, 19939, 24575, 29631, 33...</td>\n",
       "      <td>page_fi</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>78</th>\n",
       "      <td>13_of_20.pdf</td>\n",
       "      <td>64526</td>\n",
       "      <td>15</td>\n",
       "      <td>[0, 4489, 9749, 14420, 19939, 24575, 29631, 33...</td>\n",
       "      <td>page_nofi</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>13_of_20.pdf</td>\n",
       "      <td>66757</td>\n",
       "      <td>15</td>\n",
       "      <td>[0, 4490, 9751, 14423, 19943, 24580, 29637, 33...</td>\n",
       "      <td>pymupdf</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "            path full_text_len page_count  \\\n",
       "34  13_of_20.pdf         66757         15   \n",
       "58  13_of_20.pdf         64526         15   \n",
       "78  13_of_20.pdf         64526         15   \n",
       "14  13_of_20.pdf         66757         15   \n",
       "\n",
       "                                        page_char_idx      group beforeclsfix  \n",
       "34  [0, 4490, 9751, 14423, 19943, 24580, 29637, 33...   doc_nofi        False  \n",
       "58  [0, 4489, 9749, 14420, 19939, 24575, 29631, 33...    page_fi        False  \n",
       "78  [0, 4489, 9749, 14420, 19939, 24575, 29631, 33...  page_nofi        False  \n",
       "14  [0, 4490, 9751, 14423, 19943, 24580, 29637, 33...    pymupdf        False  "
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import re\n",
    "\n",
    "# VISUAL OVERVIEW\n",
    "out = out.sort_values(['path', 'group'], na_position='last')\n",
    "\n",
    "out[12:16]\n",
    "# path\tfull_text_len\tpage_count\tpage_char_idx\tgroup\tbeforeclsfix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "b9626e1c",
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "not enough values to unpack (expected 2, got 0)",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mValueError\u001b[39m                                Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[8]\u001b[39m\u001b[32m, line 72\u001b[39m\n\u001b[32m     69\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m result\n\u001b[32m     71\u001b[39m \u001b[38;5;66;03m# Usage:\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m72\u001b[39m table = \u001b[43mpercent_tables_by_path\u001b[49m\u001b[43m(\u001b[49m\u001b[43mout\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     73\u001b[39m \u001b[38;5;66;03m# table.head()\u001b[39;00m\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[8]\u001b[39m\u001b[32m, line 31\u001b[39m, in \u001b[36mpercent_tables_by_path\u001b[39m\u001b[34m(out)\u001b[39m\n\u001b[32m     28\u001b[39m \u001b[38;5;66;03m# Identify the two group names (per path, but consistent ordering for formulas)\u001b[39;00m\n\u001b[32m     29\u001b[39m \u001b[38;5;66;03m# We'll sort the column labels alphabetically as (g1, g2) for deterministic output\u001b[39;00m\n\u001b[32m     30\u001b[39m cols = \u001b[38;5;28mlist\u001b[39m(wide.columns)\n\u001b[32m---> \u001b[39m\u001b[32m31\u001b[39m g1, g2 = \u001b[38;5;28msorted\u001b[39m(cols, key=\u001b[38;5;28;01mlambda\u001b[39;00m x: \u001b[38;5;28mstr\u001b[39m(x))\n\u001b[32m     33\u001b[39m \u001b[38;5;66;03m# Pull per-path metadata back (page_count/beforeclsfix) using max across groups\u001b[39;00m\n\u001b[32m     34\u001b[39m meta = (agg.groupby(\u001b[33m'\u001b[39m\u001b[33mpath\u001b[39m\u001b[33m'\u001b[39m, as_index=\u001b[38;5;28;01mFalse\u001b[39;00m)\n\u001b[32m     35\u001b[39m           .agg(page_count=(\u001b[33m'\u001b[39m\u001b[33mpage_count\u001b[39m\u001b[33m'\u001b[39m,\u001b[33m'\u001b[39m\u001b[33mmax\u001b[39m\u001b[33m'\u001b[39m),\n\u001b[32m     36\u001b[39m                beforeclsfix=(\u001b[33m'\u001b[39m\u001b[33mbeforeclsfix\u001b[39m\u001b[33m'\u001b[39m,\u001b[33m'\u001b[39m\u001b[33mmax\u001b[39m\u001b[33m'\u001b[39m)))\n",
      "\u001b[31mValueError\u001b[39m: not enough values to unpack (expected 2, got 0)"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# Preconditions:\n",
    "# out has columns: ['path','full_text_len','page_count','page_char_idx','group','beforeclsfix']\n",
    "\n",
    "def percent_tables_by_path(out: pd.DataFrame) -> pd.DataFrame:\n",
    "    # Keep only rows with required fields present\n",
    "    req = ['path', 'group', 'full_text_len']\n",
    "    df = out.dropna(subset=req).copy()\n",
    "\n",
    "    # One row per (path, group): if multiple rows exist, choose the max length per group\n",
    "    agg = (\n",
    "        df.groupby(['path', 'group'], as_index=False)\n",
    "          .agg(full_text_len=('full_text_len', 'max'),\n",
    "               page_count=('page_count', 'max'),\n",
    "               beforeclsfix=('beforeclsfix', 'max'))\n",
    "    )\n",
    "\n",
    "    # Keep only paths that have exactly two groups\n",
    "    counts = agg.groupby('path')['group'].nunique()\n",
    "    valid_paths = counts[counts == 2].index\n",
    "    agg = agg[agg['path'].isin(valid_paths)]\n",
    "\n",
    "    # Pivot to wide: two columns named after the actual group labels\n",
    "    wide = agg.pivot(index='path', columns='group', values='full_text_len')\n",
    "\n",
    "    # Identify the two group names (per path, but consistent ordering for formulas)\n",
    "    # We'll sort the column labels alphabetically as (g1, g2) for deterministic output\n",
    "    cols = list(wide.columns)\n",
    "    g1, g2 = sorted(cols, key=lambda x: str(x))\n",
    "\n",
    "    # Pull per-path metadata back (page_count/beforeclsfix) using max across groups\n",
    "    meta = (agg.groupby('path', as_index=False)\n",
    "              .agg(page_count=('page_count','max'),\n",
    "                   beforeclsfix=('beforeclsfix','max')))\n",
    "\n",
    "    # Compute metrics\n",
    "    a = wide[g1].astype(float)\n",
    "    b = wide[g2].astype(float)\n",
    "\n",
    "    # Percent change from g1 → g2 (standard “(new-old)/old * 100”)\n",
    "    pct_change_g1_to_g2 = 100.0 * (b - a) / a.replace(0, np.nan)\n",
    "\n",
    "    # Symmetric percent difference (relative difference wrt mean), robust & signed\n",
    "    symmetric_pct_diff = 100.0 * (b - a) / ((a + b) / 2.0).replace(0, np.nan)\n",
    "\n",
    "    # Winner & ratio\n",
    "    winner = np.where(b > a, g2, np.where(a > b, g1, 'tie'))\n",
    "    ratio_best_second = np.where(a >= b, a / np.where(b == 0, np.nan, b),\n",
    "                                 b / np.where(a == 0, np.nan, a))\n",
    "\n",
    "    # Build tidy table\n",
    "    result = pd.DataFrame({\n",
    "        'path': wide.index,\n",
    "        f'full_text_len[{g1}]': a.values,\n",
    "        f'full_text_len[{g2}]': b.values,\n",
    "        f'pct_change_{g1}_to_{g2}': pct_change_g1_to_g2.values,\n",
    "        'symmetric_pct_diff': symmetric_pct_diff.values,\n",
    "        'winner_group': winner,\n",
    "        'ratio_best_second': ratio_best_second,\n",
    "    })\n",
    "\n",
    "    # Attach meta and order columns\n",
    "    result = (result.merge(meta, on='path', how='left')\n",
    "                    .sort_values(['path'])\n",
    "                    .reset_index(drop=True))\n",
    "\n",
    "    return result\n",
    "\n",
    "# Usage:\n",
    "table = percent_tables_by_path(out)\n",
    "# table.head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "49c59a8b",
   "metadata": {},
   "outputs": [],
   "source": [
    "table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e21f263f",
   "metadata": {},
   "outputs": [],
   "source": [
    "#docs_no_fillin[0]\n",
    "\n",
    "# get path doc['path']\n",
    "# get len of full text: len(doc['text'])\n",
    "# count pages --> doc['metadata']['page_char_idx'] # len of this is pages (if present)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22159a6a",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "adaparse",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
